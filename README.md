# LLM Assistant Demo

🚀 A demo project showcasing an interactive assistant built with **Next.js**, **TailwindCSS**, and deployed on **Vercel**.  
This repository serves as a playground to explore UI integration with a Large Language Model (LLM).

---

## ✨ Features
- ⚡ Built with **Next.js 14 + App Router**.
- 🎨 Styled using **TailwindCSS**.
- 🧩 Reusable UI components (People, Companies).
- 🤖 Basic assistant interaction flow.
- 🌐 Ready-to-deploy configuration for Vercel.

---
## 🚀 Demo
[Click here to try the live demo](https://llmplatform.obonelli.dev/people)

## 📦 Installation

Clone the repository and navigate into the project:

```bash
git clone https://github.com/obonelli/llm-assistant-demo.git
cd llm-assistant-demo
```

Install dependencies:

```bash
npm install
# or
yarn install
```

Run the development server:

```bash
npm run dev
```

The app will be available at:  
👉 [http://localhost:3000](http://localhost:3000)

---

## 🚀 Deployment
This project is fully compatible with [Vercel](https://vercel.com) for seamless deployment.

---

## 📂 Project Structure
```
src/
 ├─ app/          # Main routes and pages
 ├─ components/   # UI components
 ├─ mock/         # Mock data (People, Companies)
 └─ styles/       # Tailwind and style configuration
```

---

## 📜 License
This project is provided as an **educational demo**.  
Feel free to use, adapt, and experiment 🚀.
