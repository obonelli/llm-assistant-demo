# LLM Assistant Demo

ğŸš€ A demo project showcasing an interactive assistant built with **Next.js**, **TailwindCSS**, and deployed on **Vercel**.  
This repository serves as a playground to explore UI integration with a Large Language Model (LLM).

---

## âœ¨ Features
- âš¡ Built with **Next.js 14 + App Router**.
- ğŸ¨ Styled using **TailwindCSS**.
- ğŸ§© Reusable UI components (People, Companies).
- ğŸ¤– Basic assistant interaction flow.
- ğŸŒ Ready-to-deploy configuration for Vercel.

---
## ğŸš€ Demo
[Click here to try the live demo](https://llmplatform.obonelli.dev/people)

## ğŸ“¦ Installation

Clone the repository and navigate into the project:

```bash
git clone https://github.com/obonelli/llm-assistant-demo.git
cd llm-assistant-demo
```

Install dependencies:

```bash
npm install
# or
yarn install
```

Run the development server:

```bash
npm run dev
```

The app will be available at:  
ğŸ‘‰ [http://localhost:3000](http://localhost:3000)

---

## ğŸš€ Deployment
This project is fully compatible with [Vercel](https://vercel.com) for seamless deployment.

---

## ğŸ“‚ Project Structure
```
src/
 â”œâ”€ app/          # Main routes and pages
 â”œâ”€ components/   # UI components
 â”œâ”€ mock/         # Mock data (People, Companies)
 â””â”€ styles/       # Tailwind and style configuration
```

---

## ğŸ“œ License
This project is provided as an **educational demo**.  
Feel free to use, adapt, and experiment ğŸš€.
